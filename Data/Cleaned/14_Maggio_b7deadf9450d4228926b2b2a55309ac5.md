# 14 Maggio

Argomenti: Modello Translation-Based, Modello Translation-Extended
.: Yes

## Modello Translation-based (Berger e Lafferty)

$$
P(Q|A)=\prod_{w\in Q}\sum_{t\in \nu}P(w|t)P(t|A)
$$

Nel modello di linguaggio `translation-based` data una query $Q$ in input,le domande e/o risposte correlate sono classificate secondo la formula descritta accanto

Dove $Q$ è la query, $A$ la domanda o risposta correlata nel database, $V$ il vocabolario, $w$ un termine della query, $t$ un termine del vocabolario, $P(w|t)$ le probabilità di traduzione e $P(t|D)$ la probabilità normalizzata di generare $t$ dato il documento $D$.

Tale modello consente che il termine $w$ della query possa essere tradotto da altri termini $t$ che possono trovarsi nella domanda. Un problema di questo modello è che la sommatoria è estesa a tutto il vocabolario, quindi per vocabolari molto ricchi è difficile fornire una risposta in tempo reale. Il problema più grave è che il modello non garantisce che la domanda sia correlata con la query, infatti ogni termine è tradotto in maniera indipendente cioè la domanda con lo score più elevato può essere una buona traduzione termine a termine ma non può essere una buona traduzione complessiva.

## Modello di traduzione esteso di Xue

Il problema del modello `translation-based` viene risolto con il modello di `traduzione-esteso`, quello che fa è dare più peso ai termini della query rispetto alle traduzioni. Con questo modello le domande o risposte sono classificate usando la formula:

$$
P(Q|A)=\prod_{w\in Q}\frac{(1-\beta)f_{w,A}+\beta\sum_{t\in\nu}P(w|t)f_{t,A}+\mu\dfrac{c_w}{|c|}}{|a|+\mu}
$$

dove $\beta$ è un parametro compreso nell’intervallo $[0,1]$ e controlla l’influenza delle probabilità di traduzione e $\mu$ è il parametro di `smoothing` di dirchlet. Se $\beta=0$ non si ha alcun effetto del modello di traduzione, se $\beta=1$ il modello tende a quello di `berger`.

- $\beta$: è un parametro fra 0 e 1 che controlla l’influenza delle probabilità di traduzione
- $\mu$: parametro di smoothing di dirchlet, se vale 0 non si ha alcun effetto del modello di traduzione mentre se vale 1 il modello tende a quello di `berger`.
- $f_{q,D}$: numero di occorrenze del termine $q_i$ nel documento $D$
- $|D|$: numero di termini nel documento $D$
- $c_w$: numero di occorrenze del termine $w$ della query nella collezione di documenti
- $|C|$: numero totale di occorrenze di termini nella collezione.
- $A$: è la domanda/risposta o domanda-risposta

---

Fare il ranking con questi modelli può essere computazionalmente
oneroso, in quanto implicano una somma sull’intero vocabolario. Una possibile soluzione considera soltanto un numero ridotto di traduzioni per ciascun termine della query, ad esempio se si considerano le 5 traduzioni più probabili di ciascun termine, il numero di termini nella sommatoria si ridurrà da $V$ a 5.

## Probabilità di traduzione

Nel `cross-language retrieval` le probabilità di traduzione possono essere apprese automaticamente usando un corpus parallelo. Uno dei più diffusi approcci per stimare le `translation-probability` nello stesso linguaggio si basa sull’assuzione che le coppie domande/risposte formino un corpus parallelo tramite cui stimare le probabilità di traduzione più probabili.

---